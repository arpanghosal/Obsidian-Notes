## Intro :

Taken below is from a cern excerpt :
"It is necessary to establish a fresh framework that goes beyond the traditional interpretation of Higgs production rates and incorporates shape information, Higgs self-interaction rates and other SM processes including Electroweak precision data and also analyses of processes including top quarks production.

The absence of definitive signals indicating physics beyond the SM at the LHC suggests the possibility of a scale separation between the SM and any potential new physics at higher energies. This motivates the utilization of the Standard Model Effective Field Theory (SMEFT) [[20](https://arxiv.org/abs/1706.08945)] as a valuable tool for indirectly searching for new physics in LHC data [read also a previous [EP news article](https://ep-news.web.cern.ch/content/lhc-takes-smeft-challenge)]. The SMEFT offers the advantages of (near) model-independence, systematic improvement capabilities, and the ability to simultaneously leverage multiple datasets.

Effective Filed theories introduce new-physics states at a high mass scale Λ, significantly larger than the electroweak scale. By expanding in terms of E/Λ, where E represents the typical energy exchanged in a process, the theory provides predictions for experimental observables. This expansion is achieved through a series of operators, which are constructed as gauge-invariant combinations of SM fields with energy dimensions greater than four.

By measuring observables that are sensitive to the effects of SMEFT operators, it becomes possible to constrain the values of c(d)/Λ4d, where c(d)) represents the Wilson coefficients associated with the dimension-d operators O(d). The leading effects of new physics are expected to be captured by dimension-six operators, as higher-dimensional operators are suppressed by greater powers of Λ−1. The extensive measurements conducted by ATLAS, focusing on electroweak interactions, the Higgs boson, and top quarks, exhibit sensitivity to a wide range of operators that impact various aspects of particle interactions. These operators influence Higgs boson couplings, self-interactions of weak bosons, couplings between weak bosons and fermions, as well as four-fermion couplings. To achieve the best possible sensitivity and disentangle the effects of these operators, it is necessary to combine a diverse set of measurements. A first measurement has been performed by the ATLAS Collaboration with the Run2 data [[21](https://cds.cern.ch/record/2816369/files/ATL-PHYS-PUB-2022-037.pdf)]. Also CP violating effects can be included.

Conducting a global analysis of constraints on the Wilson coefficients of the SMEFT is of utmost importance when allowing more than a few Wilson coefficients to be non-zero. This is due to the fact that numerous SMEFT operators contribute to multiple observables, emphasizing the need to avoid analyzing different measurement classes in isolation. This feature becomes increasingly important as the statistics of the ATLAS data increases. During the analysis of Run 3, a significant emphasis will be placed on utilizing Effective Field Theories (EFTs). This endeavor holds particular significance as it aims to uncover potential deviations from the Standard Model (SM) that could be amplified through the comprehensive interpretation of diverse measurements. Furthermore, the outcomes of these investigations will play a crucial role in guiding our future research endeavors at the HL-LHC."

---
## Basics of EFT (with examples):

Effective Field Theory (EFT) is a powerful framework used in theoretical physics to describe phenomena at a given energy scale, while "integrating out" or ignoring the details of the underlying higher-energy or shorter-distance scales. This concept can be applied to a wide range of contexts, from particle physics to condensed matter.

### Basic Idea

Imagine you're watching a movie on an old TV screen. If you stand too close, you see individual pixels, but if you step back, you see a clear picture. EFT works similarly. Instead of focusing on every detail (or every pixel), it gives us the big picture.

In EFT, we ignore the details of some high-energy interactions and only focus on long-range or low-energy processes.

### Formalism

The idea is to express the Lagrangian (a mathematical object that describes the dynamics of a system) in terms of fields that are relevant at the scale of interest. The Lagrangian will have a series of terms, organized by their dimension:

LEFT=LSM+1ΛL5+1Λ2L6+…LEFT​=LSM​+Λ1​L5​+Λ21​L6​+…

Here:

- LSMLSM​ is the Standard Model Lagrangian or some other low-energy Lagrangian.
- ΛΛ is the "cutoff" scale, where new physics might appear.
- L5L5​, L6L6​, etc., are operators of higher dimension which are suppressed by the corresponding powers of ΛΛ.

### Application

Let's discuss one of the most famous examples: Fermi's theory of beta decay. Before the discovery of the W and Z bosons, Fermi described weak interactions in terms of a four-fermion interaction. This interaction looked like direct contact between particles, with no mediator.

The Fermi interaction had the form:

LFermi=GF2ψˉνγμ(1−γ5)ψeψˉnγμ(1−γ5)ψpLFermi​=2

​GF​​ψˉ​ν​γμ(1−γ5​)ψe​ψˉ​n​γμ​(1−γ5​)ψp​

Where GFGF​ is Fermi's constant, and the ψψ terms represent various fermion fields.

Later, it was discovered that this interaction was actually mediated by the exchange of a W boson. However, for energies much less than the W boson mass, the Fermi theory provides a good approximation.

### Why is EFT Powerful?

1. **Separation of Scales**: We can effectively deal with physics at a particular scale without knowing the details of higher scales.
    
2. **Universality**: EFTs can describe a broad class of underlying theories, as long as they have the same symmetries and particle content at low energies.
    
3. **Systematic Improvements**: If we want to refine our predictions, we can systematically include higher-dimensional operators, suppressed by higher powers of the scale ΛΛ.
    

### Limitations

- **Accuracy**: The accuracy is restricted to a specific range of energy scales.
    
- **New Physics**: EFT cannot predict new particles or phenomena but can hint at their presence through deviations from expected results.
    

### In Conclusion

Effective Field Theories are a cornerstone of modern theoretical physics, providing a systematic way to describe phenomena when a full microscopic description is either unknown or too complicated to handle. They are widely used in particle physics, nuclear physics, and condensed matter physics. The ability to "integrate out" high-energy details and focus on relevant low-energy processes is what makes EFTs incredibly versatile and powerful.


$$ \LARGE{ \textbf{LLPs also form some part.}}$$

#llp #lhc [[LLP]]   : see here.

----


## Description
In the context of Beyond the Standard Model (BSM) physics, the approach often involves considering the possible existence of heavier, as-yet-undiscovered particles. These particles, though not directly observable with current technology, might still influence the interactions and behaviors of particles that are part of the Standard Model (SM). To account for these potential effects, new terms are added to the SM Lagrangian. These terms typically manifest as higher-dimensional operators, often of dimension-six or higher, which describe the hypothesized interactions between BSM particles and known SM particles.

### Experimentally Finding and Measuring These Couplings

1. **High-Energy Particle Collisions**: Experiments at particle accelerators, like the Large Hadron Collider (LHC), can provide insights. By colliding particles at very high energies, physicists can probe for deviations from the SM predictions that might hint at the influence of BSM physics.

2. **Precision Measurements**: Precise measurements of known processes can reveal discrepancies with SM predictions. For instance, precise measurements of the magnetic moment of particles, decay rates, or scattering cross-sections can show deviations that might be attributable to higher-dimensional operators.

3. **Parameterization**: In theoretical models, these new couplings are often parameterized in terms of their strength and the energy scale at which they become relevant. The coefficients of these higher-dimensional operators (which represent the strength of the new couplings) are crucial parameters. They are typically expressed as a ratio where the numerator represents the strength of the interaction, and the denominator includes the new physics energy scale, often represented by Λ.

### Theoretical Values and Conjectures

1. **Effective Field Theory (EFT)**: The framework used to describe these interactions is known as Effective Field Theory. EFT allows physicists to systematically include the effects of high-energy physics in low-energy observations by adding higher-dimensional operators to the SM Lagrangian.

2. **Theoretical Predictions**: Theoretical values for the coefficients of these new terms are often based on various BSM theories. These theories might suggest specific forms for the new interactions and provide a range of expected values for their strengths.

3. **Matching and Running**: Theoretical approaches like 'matching' (connecting BSM theories with the effective theory at a high energy scale) and 'running' (using renormalization group equations to understand how these parameters change with energy) are employed to make predictions.

4. **Conjectures and Hypotheses**: It's important to note that many of these theoretical values are conjectural. They are based on extensions of the SM that have not yet been experimentally confirmed. They serve as guidelines for what physicists might expect to find in experiments.

In summary, the search for BSM physics involves both theoretical predictions based on extensions of the SM and experimental efforts to find deviations from SM predictions. The process of identifying these new couplings and measuring their strengths is ongoing and represents a significant part of current research in particle physics.



---

## Conflating EFT and renormalization : 
Effective Field Theory (EFT) and renormalization are deeply connected concepts, and they share some similarities in philosophy, but they are used for different purposes.

### Renormalization

Renormalization is a procedure to deal with the divergences that arise in the perturbative evaluation of quantum field theory (QFT) observables, especially in the context of the Standard Model (SM). When we calculate certain quantities in QFT using Feynman diagrams, we sometimes encounter infinite results, which are physically nonsensical. Renormalization provides a systematic way to absorb these infinities into the redefinition of parameters (like masses and couplings) of the theory, making predictions finite and physically meaningful.

### EFT and Renormalization

Effective Field Theories are based on the same basic idea as renormalization: separating different energy scales. Just like in renormalization, where we absorb high-energy or short-distance information into redefined parameters, in EFT we "integrate out" the high-energy or short-distance degrees of freedom to focus on the low-energy or long-distance physics.

However, while renormalization is about handling infinities and making QFT calculations meaningful, EFT is about simplifying the description of a system by focusing on the relevant degrees of freedom at a particular energy scale.

### Relationship with the Standard Model

When we use EFT in the context of the SM, it's often to describe physics that might exist beyond the SM. Since we don't know the exact form of a potential "Beyond the Standard Model" (BSM) theory, we use EFT to parametrize our ignorance. We add to the SM Lagrangian all possible higher-dimensional operators consistent with the SM symmetries, suppressed by powers of a scale ΛΛ which is the energy scale of the new physics.

As we perform calculations in this EFT, we might encounter divergences. Here's where renormalization comes in again. The renormalization process in EFT can give us insights about the structure of the UV (ultraviolet, or high-energy) completion of the theory - the more fundamental theory that the EFT is an approximation of.

### In Summary

- **Renormalization** is a technique to handle infinities in QFT calculations.
- **EFT** is a framework to describe physics at a given energy scale by integrating out higher energy scales.
- Both concepts involve the idea of separating or handling different energy scales in a QFT.
- EFT allows for a parametrized description of unknown higher-scale physics, like BSM physics. When performing calculations in such an EFT, renormalization techniques might be needed.
### Relationship with Renormalization:

When we do loop calculations in an EFT, we again encounter divergences, but some of these can be absorbed by redefining the coefficients of the higher-dimensional operators (similar to how we redefine mass and charge in QED). If an EFT is well-defined, any observable we compute should be independent of the scale at which we separate "low-energy" from "high-energy."

For instance, when doing calculations in the Fermi theory, we'd encounter divergences that would be absorbed by redefining the Fermi constant, GFGF​. However, if we were doing the calculations in the full theory with the W boson, these divergences would be absorbed into the definitions of other parameters, like the W boson mass or the weak coupling constant.

In essence, while both renormalization and EFT involve separating or handling different energy scales, the former deals directly with the divergences of QFT, while the latter offers a simplified description of phenomena by ignoring high-energy details. Both are crucial for making meaningful and accurate predictions in particle physics.


---


$\[ e(\mu) = Z_e(\mu) e_{\text{bare}} \]$

## Renormalization

In quantum field theory, infinities might arise when you calculate certain quantities. Renormalization is a procedure to rid the theory of these infinities.

The basic idea is:

$$ e(\mu) = Z_e(\mu) e_{\text{bare}} $$

$$ e(\mu) = Z_e(\mu) e_{\text{bare}} $$

Where:
- $e(\mu)$ is the renormalized charge.
- $e_{\text{bare}}$ is the bare charge.
- $Z_e(\mu)$ is the renormalization constant.

## Effective Field Theory (EFT)

EFT is a way to describe physical phenomena at a particular scale. The Lagrangian for an EFT is:


$$ \mathcal{L}_{\text{EFT}} = \mathcal{L}_{\text{SM}} + \sum_i \frac{c_i}{\Lambda^{d_i}} \mathcal{O}_i $$


Where:
- $\mathcal{L}_{\text{SM}}$ is the Standard Model Lagrangian.
- $\mathcal{O}_i$ are operators representing new interactions.
- $c_i$ are the coefficients.
- $\Lambda$ is the scale of new physics.
- $d_i$ is the dimension of the operator $\mathcal{O}_i$.

nice explanation by Knut in his atlas lecture series first talk :
https://indico.cern.ch/event/1338871/attachments/2745421/4782252/video1080087758.mp4


---


## Integrating out heavy terms in SM L:
The passage you've shared relates to a concept in high-energy physics, particularly in the context of the Standard Model (SM) of particle physics. It's a bit technical, so I'll break it down step by step:

1. **High-Energy Physics and Heavy Degrees of Freedom**: In particle physics, "degrees of freedom" generally refer to independent physical parameters that define the state of a system. In high-energy physics, these might include particles or fields that are too massive (or "heavy") to be directly observed or produced in current experiments.

2. **Integrating Out Heavy Degrees of Freedom**: When these heavy particles cannot be directly observed or produced (due to limitations in energy), physicists use a technique where they "integrate out" these heavy degrees of freedom. This means they mathematically remove these variables from the equations that describe particle interactions, simplifying the model while still accounting for the effects these heavy particles would have.

3. **New Terms in the SM Lagrangian**: The Standard Model is described by a mathematical structure called the Lagrangian, which encapsulates the dynamics of particles and fields. When heavy degrees of freedom are integrated out, new terms are added to this Lagrangian. These new terms are made up of "higher-dimensional operators" - mathematical entities that describe interactions not accounted for in the original framework of the SM.

4. **Suppression by Λ**: The influence of these new terms is suppressed by the inverse power of a scale Λ (Lambda), which is characteristic of the energy scale of the heavy particles. The higher the energy scale, the less significant these terms are at lower energies.

5. **Respecting Symmetries and Conservation Laws**: These new terms still respect the fundamental symmetries of the Standard Model, such as Lorentz invariance (the laws of physics are the same regardless of how you're moving) and SM gauge symmetries (related to the fundamental forces), as well as conservation laws like baryon and lepton number conservation.

6. **Effects After Electroweak Symmetry Breaking (EWSB)**: After EWSB, a key process in particle physics where the electromagnetic and weak nuclear forces become distinct, these integrated-out terms lead to new couplings - new ways in which particles can interact. These new interactions are not part of the SM at tree level (the simplest level of particle interaction diagrams).

7. **New Interactions**: This can include interactions like electric and magnetic dipole moments (properties of particles that describe how they interact with electromagnetic fields) that don't exist in the basic SM, as well as modifications to existing SM interactions.

In summary, integrating out heavy degrees of freedom is a way to simplify the models of particle physics while still accounting for the potential effects of particles and fields that cannot be directly observed or produced at current energy levels. This leads to modifications of the Standard Model, which can have observable consequences at lower energies.


-----

# Cedm and Cmdm:

## Definitions:
- **cEDM (chromoelectric dipole moment)**: It is a measure of the distribution of charge within a particle in the presence of a chromoelectric field (related to the strong force in quantum chromodynamics). It represents a CP-violating property, meaning it differs under the combined operations of charge conjugation (C) and parity (P) transformation.
    The CEDM contribution in the SM arises from the three-loop diagrams and has been shown to be very small. 
- **cMDM (chromomagnetic dipole moment)**: It quantifies the distribution of magnetic properties within a particle under the influence of a chromomagnetic field (again related to the strong force). It's analogous to the magnetic dipole moment in electromagnetism but in the context of the strong nuclear force.
## Values:
In the context of the Standard Model of particle physics, both the chromoelectric dipole moment (cEDM) and the chromomagnetic dipole moment (cMDM) of particles are expected to be very small, but there are some key differences in their theoretical expectations:

1. **cEDM**: The cEDM is particularly interesting because it violates CP symmetry (the combination of charge conjugation and parity). In the Standard Model, CP violation is a very rare phenomenon, observed in certain weak interactions but not in strong interactions. As a result, the cEDM, which would represent CP violation in the context of the strong force (quantum chromodynamics, QCD), is expected to be extremely small or even zero within the Standard Model framework.
    
2. **cMDM**: The cMDM, analogous to the magnetic dipole moment but for the strong force, doesn't necessarily imply CP violation. It can exist within the Standard Model without violating any fundamental symmetries. However, the value of cMDM in the Standard Model is also expected to be quite small, primarily due to the strong force's inherently short range and the confining nature of QCD.


----

# gtt vertex and the effects: 
from roknabadi and estesami

![[Pasted image 20231128104545.png]]

The context here is the calculation of the chromomagnetic dipole moment (CMDM) of the top quark in the Standard Model (SM), particularly in relation to the \( gtt \) vertex, where \( g \) stands for a gluon and \( tt \) represents the top quark-antiquark pair. This involves evaluating Feynman diagrams that contribute to the CMDM at one-loop level. Let's break down the key points:

1. **CMDM of the Top Quark**: The CMDM is a quantum property of quarks, akin to the magnetic dipole moment but in the realm of the strong interaction. For the top quark, this property can have significant contributions because the top quark is the heaviest elementary particle, and its interactions might include effects beyond the simplest predictions of the SM.

2. **One-Loop Diagrams**: The calculation involves considering corrections to the \( gtt \) vertex at the one-loop level. In quantum field theory, one-loop diagrams are a first-order approximation beyond the simplest (tree-level) interaction. They can provide significant corrections to the properties of particles, especially for the top quark.

3. **QCD and Electroweak Contributions**: These one-loop diagrams include both Quantum Chromodynamics (QCD) and electroweak interactions. QCD is the theory of the strong force, which governs the interactions of quarks and gluons, while electroweak interactions involve the weak force and electromagnetism.

4. **Types of QCD Diagrams**:
    - **Diagram with External Gluon Emitted from Internal Top Quark**: This Feynman diagram represents a process where the top quark emits and reabsorbs a gluon (the mediator of the strong force), and an additional gluon (the external gluon) is emitted from this internal top quark line.
    - **Diagram with External Gluon from Exchanged Gluon**: This involves the non-Abelian nature of QCD (meaning gluons can interact with each other). In this diagram, the top quark exchanges a gluon with another quark or gluon, and the external gluon is emitted from this exchanged gluon. This is a unique feature of QCD, differentiating it from electromagnetism where photons do not interact with each other.

5. **Non-Abelian Properties of Strong Interaction**: QCD is a non-Abelian gauge theory, meaning that its force carriers (gluons) can interact with each other. This property leads to complex dynamics and is crucial in the behavior of the strong force.

The CMDM of the top quark is a subject of interest because its measurement and theoretical calculation can provide insights into the SM and potential new physics. Deviations from the predicted CMDM value could indicate physics beyond the SM, making precise calculations and measurements important for the ongoing exploration of fundamental particle physics.


When discussing the "effects" that might be included in the interactions of the top quark beyond the simplest predictions of the Standard Model (SM), we are referring to various potential phenomena or modifications in particle behavior that are not fully accounted for in the current framework of the SM. These effects can be diverse in nature and may include:

1. **New Types of Interactions**: Beyond the known electromagnetic, weak, and strong forces, there could be additional forces or interaction types that have not yet been discovered. These new interactions might become apparent only under certain conditions, such as at the high energy scales associated with the top quark.

2. **Anomalous Couplings**: The top quark might exhibit anomalous couplings that differ from those predicted by the SM. This could include unexpected strengths or forms of its interactions with other particles, like gluons, W/Z bosons, or the Higgs boson.

3. **Higher-Order Effects**: The top quark's large mass makes it susceptible to significant loop-level corrections (quantum effects that occur at higher orders in perturbation theory). These effects can modify the properties and interactions of the top quark in ways not fully predicted by the tree-level (simplest) calculations of the SM.

4. **CP Violation**: While the SM does include CP violation (where a process does not produce the same outcome if both charge and parity are inverted), the amount of CP violation predicted by the SM is insufficient to explain certain observed phenomena, like the matter-antimatter asymmetry in the universe. The top quark, due to its unique properties, might exhibit additional CP-violating effects.

5. **Mixing with Beyond Standard Model (BSM) Particles**: The top quark might mix or interact with hypothetical particles predicted by theories beyond the SM, such as supersymmetric particles or other exotic forms of matter. These interactions could alter the decay patterns, production rates, or other observable properties of the top quark.

6. **Quantum Gravity Effects**: At the energy scales relevant to the top quark, some theories suggest that effects of quantum gravity might start to become significant. While this is highly speculative, it's a potential area of exploration.

7. **Flavor Physics**: The top quark plays a unique role in the flavor physics of quarks (how different types of quarks transform into each other). There might be effects related to flavor that are not fully captured by the SM, especially considering the top quark's role in electroweak symmetry breaking.

In summary, the nature of these effects could range from modifications in the strength and form of known interactions, to entirely new phenomena that would require extensions to the SM. Studying the top quark is particularly interesting because its large mass suggests it could be sensitive to such new physics, making it a key focus in particle physics research.

-----

# DM by DAMA/XENON: 
#darkmatter

The DAMA (DArk MAtter) experiment, specifically DAMA/LIBRA, is an experimental physics setup located at the Gran Sasso National Laboratory in Italy. Its main goal is to detect dark matter particles through their interactions with ordinary matter in a highly sensitive, low-background environment. DAMA/LIBRA has reported an annual modulation in the rate of detected events, which they interpret as evidence for the presence of dark matter particles. This modulation is consistent with what one might expect as the Earth moves through the halo of dark matter in our galaxy, theoretically changing the rate of dark matter interactions.

However, the claims made by DAMA have been controversial for several reasons, leading to skepticism and efforts to verify or refute their results. Here's why DAMA's experimental results regarding dark matter might be considered possibly wrong or at least not universally accepted:

### 1. Lack of Replication

Other experiments designed to detect dark matter, such as XENON, LUX, and PandaX, have not observed the same annual modulation in their data. These experiments use different target materials and detection techniques, which could potentially explain the discrepancy, but the lack of replication casts doubt on the DAMA results.

### 2. Issues with Experimental Methodology

Critics have pointed out potential issues with DAMA's experimental setup and analysis, including questions about the calibration of detectors, background radiation control, and statistical analysis methods. These concerns raise doubts about the reliability of the observed signal as evidence for dark matter.

### 3. Theoretical Challenges

DAMA's interpretation of their results relies on specific properties of dark matter particles and their interactions with ordinary matter. However, the lack of corroborating evidence from other types of experiments, including collider experiments and indirect dark matter searches, challenges the specific dark matter models that would be consistent with DAMA's findings.

### Ongoing Work to Prove/Disprove DAMA's Results

Given the controversy, there is significant ongoing work aimed at testing the DAMA results:

- **Replication Efforts**: Several experiments have been proposed or are underway to replicate the DAMA experiment using the same target material (sodium iodide) but with different setups and improved background suppression. Examples include COSINE-100, ANAIS, and SABRE.
  
- **Technological Advances**: Improvements in detector technology, data analysis techniques, and background radiation shielding are helping to increase the sensitivity of dark matter detection experiments. These advances might eventually provide a clear verification or refutation of the DAMA results.

- **Theoretical Investigations**: Theoretical physicists are exploring a wide range of dark matter models and their implications for direct detection experiments. This work includes examining how different types of dark matter particles might interact with the detector materials used by DAMA and other experiments.

- **Complementary Searches**: Searches for dark matter through other methods, such as indirect detection (looking for signals from dark matter annihilation or decay in space) and collider experiments (attempting to produce dark matter particles directly), provide additional pathways to test the validity of DAMA's interpretation.

### Xenon:
The XENON experiment uses a dual-phase xenon detector to search for dark matter. When a dark matter particle collides with a xenon nucleus, it creates light and free electrons. These electrons are then amplified in the gas phase above the liquid xenon, creating more light. Photomultiplier tubes detect this light, allowing for event reconstruction. The experiment distinguishes potential dark matter signals from background noise using the light's characteristics and is conducted underground to minimize interference from cosmic rays.

## Three ways of detect DM particles:
![[Pasted image 20240311112225.png]]

More here: 
#link
https://indico.desy.de/event/42884/contributions/159245/attachments/89026/119545/Marrodan_DPG2024.pdf

#paper

https://arxiv.org/pdf/2002.00459.pdf - why DAMA results might be wrong.
"We point out that a slowly varying rate can bias or even simulate an annual modulation if data are analyzed in terms of residuals computed by subtracting approximately yearly averages starting from a fixed date, rather than a background continuous in time. ""


----


# Renormalizability in EFT:

The statement that "effective field theories (EFTs) are not valid at small length scales, so they need not be renormalizable" touches on some key concepts in theoretical physics, specifically in the domain of quantum field theory. Let's break down these concepts to clarify what's meant by this statement.

### Effective Field Theories (EFTs)

An effective field theory is a type of quantum field theory designed to be effective only up to certain energy scales or length scales. Beyond this scale, the theory does not claim to provide a correct description of physical phenomena. This approach is useful because it allows physicists to focus on the relevant physics at accessible energy scales without needing a complete understanding of the underlying, more fundamental theory (which may be valid at much higher energies).

### Renormalizability
#renormalize
#bsm #eft
Renormalizability is a property of a field theory that dictates whether the infinities that arise in the calculations of the theory’s predictions can be systematically removed by a finite number of adjustments (or "renormalizations") to the theory’s parameters (like mass and charge). Historically, a theory being "renormalizable" was seen as essential for it to be physically meaningful because it suggested the theory could be used predictively at all energy scales.

- **Renormalizable Theories**: These theories, like Quantum Electrodynamics (QED), involve interactions described by dimensionless coupling constants and remain predictive at very high energies. In renormalizable theories, all divergences that arise in calculations can be absorbed into a finite number of parameters.

- **Non-Renormalizable Theories**: These involve terms in the Lagrangian with dimensionful parameters, indicating dependence on a specific scale. As a result, new types of infinities typically appear at high energies that cannot be absorbed just by the original parameters of the theory, suggesting the theory breaks down at those scales.

### Context of EFTs and Renormalizability

In the context of effective field theories, the term "renormalizable" takes on a specific significance:
- **Validity at Small Length Scales**: EFTs are generally not intended to be valid at arbitrarily high energies (or equivalently, small length scales). They are designed to provide accurate descriptions only up to a certain energy scale (the cutoff scale). Above this scale, new physics—potentially described by another, more fundamental theory—is expected to take over.
- **Need for Renormalizability**: Since EFTs are not expected to be valid beyond their cutoff scale, they do not need to be renormalizable in the traditional sense. The theory only needs to make sense and match experimental data below this cutoff. Any non-renormalizable divergences that would occur at higher energies are considered irrelevant because the theory does not claim to be applicable at those energies.

In practice, EFTs include both renormalizable and non-renormalizable terms (the latter often representing higher-order corrections). These terms are organized in a way that respects the symmetries of the problem and are used to make predictions within the valid energy range of the EFT. This methodology allows physicists to effectively describe phenomena without a complete theory of everything, particularly in areas like particle physics, condensed matter physics, and nuclear physics.

So, when discussing EFTs in the context of "renormalizable groups," it generally involves discussions around how these theories handle different terms and interactions within their valid scopes, not necessarily implying that the EFTs themselves must be renormalizable in the traditional whole-scale sense.

## As a QED example:
The concept of infinities being absorbed into parameters within the context of quantum field theory primarily pertains to the mathematical process known as "renormalization." This process addresses the issue of infinities that often arise in calculations involving quantum fields and interactions. These infinities do not represent physical realities but are artifacts of the theories and the methods used to calculate physical quantities like mass, charge, and the strength of interactions. Let's clarify this concept through an example, focusing on the renormalization of mass in a simple theory.

### Example: Renormalization of Mass in Quantum Electrodynamics (QED)

Quantum Electrodynamics (QED) is a theory that describes how light and matter interact and is one of the first theories where renormalization was extensively applied. In QED, calculations involving interactions between electrons and photons can lead to infinite results. Here’s how renormalization works in this context:

1. **Bare Mass and Physical Mass**: In the theory, electrons have a "bare mass" which is the mass term included in the original Lagrangian (the formula that describes the dynamics of the field). This is not directly observable. Through interactions, especially those involving virtual particles (like an electron emitting and then reabsorbing a photon), the electron's mass receives corrections that can lead to infinite values in theoretical calculations.

2. **Infinities in Calculations**: When calculating how an electron’s mass gets corrected due to its interactions with photons (a process called self-energy), the integrals used can diverge to infinity. This typically happens because the calculations include contributions from all possible energies of the virtual photons, including infinitely high energies.

3. **Absorption into Parameters**: The renormalization process manages these infinities by absorbing them into the bare mass. Here’s the step-by-step:
   - **Regularization**: First, a method like cutoff regularization is applied, where the integrals are artificially limited to some maximum energy level (cutoff). This controls the infinities temporarily.
   - **Renormalization**: The infinite part of the calculated mass correction is then conceptually added to the bare mass. This redefinition shifts the bare mass by an infinite amount in such a way that when you calculate the observed (or physical) mass, it remains finite and matches experimental values.

4. **Resulting Physical Mass**: The result is that the infinite corrections are canceled out by the infinite adjustments made to the bare mass. The observable physical mass of the electron is what's left, and it matches what we measure in experiments.

### Physical Meaning

The physical interpretation of this process is not that an actual infinite quantity is being transferred or "sucked up" by real particles. Instead, renormalization is a method to handle the limitations and peculiarities of the mathematical models used in quantum field theory. It ensures that the predictions of the theory are finite, well-defined, and in agreement with what we observe in nature. The "absorption" of infinities into parameters is a mathematical trick that allows theorists to remove non-physical infinities and make meaningful, empirical predictions.

This process highlights both the power and the abstract nature of modern theoretical physics, where mathematical structures can provide deep insights into the workings of the universe, albeit sometimes through complex and non-intuitive methodologies.

## On antiparticles:

The issue surrounding the infinite corrections to the electron's mass and other related quantum phenomena did contribute to significant theoretical developments, including Paul Dirac's prediction of antimatter. However, it's important to clarify how Dirac's ideas were specifically related to his formulation of the theory rather than just the problem of infinities.

### Dirac's Equation and the Prediction of Antimatter

Paul Dirac's work, leading up to his prediction of antimatter, was primarily motivated by the need to reconcile quantum mechanics with special relativity. The infinite corrections to electron properties (like mass and charge) due to interactions described in quantum electrodynamics (QED) were indeed troubling, but they were part of a broader set of issues that theorists were grappling with in the early 20th century.

1. **Dirac Equation (1928)**: Dirac developed a version of the quantum theory that was consistent with special relativity, leading to what is now known as the Dirac Equation. This was a groundbreaking development because it provided a relativistic equation for the electron.

2. **Prediction of Antimatter**: An intriguing aspect of the Dirac Equation was that it naturally included solutions that had negative energy states. Initially, these negative energy solutions were perplexing and considered problematic. Dirac's insight was to propose that these could represent a new type of particle, identical in mass to the electron but opposite in charge. He speculated that the "empty" negative energy states were normally filled (forming a "sea" of negative energy electrons), and that a positive energy electron could move to fill a negative energy state, leaving behind a "hole." This hole would behave as if it were a particle with positive charge — an antielectron, or positron.

3. **Experimental Confirmation**: The prediction was confirmed in 1932 when Carl Anderson discovered the positron during experiments with cosmic rays. This discovery was a monumental verification of Dirac's theory and marked the first identification of antimatter.

### Relationship with Renormalization

While Dirac's antiparticle theory addressed the negative energy solutions of his equation, the broader issues of infinities in quantum field theory continued to be problematic. These were not fully addressed until the development of renormalization techniques in the 1940s and 1950s by theorists like Richard Feynman, Julian Schwinger, and Sin-Itiro Tomonaga, which successfully made sense of infinite corrections in QED.

Dirac's own views on renormalization were actually quite critical. He was not satisfied with the renormalization approach, viewing it as a mathematical "patch" rather than a satisfactory resolution to the fundamental issues of the theory. Dirac favored a more radical rethinking of the theory that could naturally avoid infinities without such adjustments, though such a reformulation never materialized from his work.

In summary, while Dirac's work on the electron and his prediction of antimatter were pivotal moments in theoretical physics, they were part of a broader context involving the struggle to integrate quantum mechanics with relativity and the ongoing challenge of infinite quantities in field theories. The solution to these infinities through renormalization, although later, transformed the landscape of particle physics and remains a standard part of the theoretical toolkit.

## On bare existence:

### Interactions of Electrons with Virtual Particles

In quantum field theory, particles like electrons do not exist in isolation, even in seemingly empty space. This is because of the inherent nature of quantum fields to fluctuate randomly, leading to the temporary creation and annihilation of "virtual particles." These are not real particles in the conventional sense; instead, they are transient excitations of the field that appear and disappear within the constraints of the uncertainty principle.

#### Virtual Particles and Electron Mass

1. **Self-Energy**: When considering an electron in QFT, its interactions with these virtual particles, particularly virtual photons (the force carriers of the electromagnetic field), can affect its properties. This is most notably seen in the self-energy of the electron, which is a correction to its mass due to these interactions. The electron emits and reabsorbs virtual photons, which theoretically can alter its observed mass.
    https://quantummechanics.ucsd.edu/ph130a/130_notes/node475.html

2. **Infinite Corrections**: Theoretically, these corrections to the electron's mass (and other properties) from interactions with virtual particles can be infinitely large due to contributions from all energy scales. This problem of infinities was one of the major hurdles in early quantum field theory and was only resolved with the development of renormalization techniques.

3. **Renormalization**: This process involves absorbing these infinite corrections into the "bare" parameters (like the bare mass) of the theory, allowing for the calculation of finite, observable quantities. The physical, or "renormalized," mass of the electron is what we measure in experiments, and it is finite.


The statement that contributions to the mass of a particle can come from "all energy states" in quantum field theory (QFT) is indeed tied closely to how virtual particles and quantum fluctuations behave, as well as the principles outlined by the Heisenberg Uncertainty Principle (HUP). This concept reflects the profound effects of quantum mechanics on particle physics, especially in how particles interact and acquire mass.

### Virtual Particles and Energy States

1. **Virtual Particles**: In QFT, virtual particles are temporary fluctuations that arise spontaneously from the vacuum due to quantum uncertainty. They are integral to interactions between particles, such as the force-carrying exchanges in the Standard Model of particle physics. Unlike real particles, virtual particles do not need to obey the same conservation laws (such as conservation of energy) for extended periods—they only need to satisfy these laws when viewed over very short times, as permitted by the HUP.

2. **Heisenberg Uncertainty Principle (HUP)**: The HUP in the context of energy and time allows that \(\Delta E \Delta t \approx \hbar\), where \(\Delta E\) is the uncertainty in energy and \(\Delta t\) is the uncertainty in time. This relationship implies that for very short times, \(\Delta t\), there can be significant fluctuations in energy, \(\Delta E\). This allows virtual particles to have a wide range of energies—even very high energies—provided they exist only for a very short time.

### Contributions to Mass from All Energy States

1. **Quantum Corrections**: In the calculations of a particle's properties, such as mass, quantum corrections arising from virtual particles play a significant role. These corrections involve integrating over all possible energy states that these virtual particles could occupy. Because of the HUP, there is theoretically no upper limit to the energy that these virtual particles might momentarily possess. 

2. **Integrating Over All Energies**: When calculating the self-energy of a particle (a quantum correction to its mass), contributions from virtual particles at all possible energy states are considered. This integral over energy states can lead to divergences—infinities—which are then handled by renormalization.

3. **Physical Interpretation**: Conceptually, this means that a particle's mass is influenced by interactions with virtual particles spanning a continuum of energy scales. These interactions include not just those within the familiar range of energies but also those involving extraordinarily high energies, well beyond the particle’s own energy scale. 

### Example: Electron Self-Energy

- **Electron in Quantum Electrodynamics (QED)**: The corrections to the electron’s mass involve contributions from virtual photons and electron-positron pairs that can have any amount of energy, as allowed by quantum mechanics. The higher the energy of these virtual particles, the shorter the time they exist.

### Implications and Limitations

The necessity to integrate over all these energy states and handle the resulting infinities through renormalization is one of the critical aspects that distinguish quantum field theories from classical theories. It shows how quantum mechanics fundamentally alters our understanding of particle properties, introducing a dependence on the entire structure of the field theory, including aspects that might occur at scales far beyond direct experimental reach.

This concept underscores the interconnectedness of scales in quantum field theory and the complex, often counterintuitive nature of quantum corrections.


### Quantum Fluctuations and Historical Context

Quantum fluctuations refer to the temporary changes in the amount of energy in a point in space, as allowed by the Heisenberg uncertainty principle. These fluctuations underlie the appearance of virtual particles and are a fundamental feature of the quantum vacuum.

#### Historical Development

- **Early 20th Century**: The concept of the quantum vacuum and fluctuations began to form with the development of quantum mechanics in the 1920s and the later formalization of QFT in the 1930s and 1940s. The idea that the vacuum could have complex properties involving virtual particles became clearer with these developments.
  
- **Casimir Effect**: The theoretical prediction of the Casimir effect by Hendrik Casimir in 1948 and its experimental verification in the 1950s provided one of the first tangible proofs of quantum fluctuations in the vacuum. This effect, where two uncharged parallel plates attract each other in a vacuum, arises from the vacuum fluctuations of the electromagnetic field.

So, while early physicists like Dirac were certainly aware of complex quantum phenomena, the detailed understanding of quantum fluctuations and their implications for the vacuum state developed more fully in the mid-20th century with advances in QFT and experimental validations like the Casimir effect.

### Conclusion

In essence, an electron never truly exists "on its own" due to its continuous interaction with the quantum fields that permeate all of space. This interaction with virtual particles is an unavoidable consequence of the quantum nature of the universe. The treatment of these interactions and the resulting infinities has shaped much of modern particle physics and continues to be an area of rich theoretical investigation.


## Why EFTs?
Yes, your description captures the essence of how effective field theories (EFTs) are utilized in physics. EFTs are indeed pragmatic tools in theoretical physics, designed to provide "good enough" models for describing physical phenomena within a certain range of energies or scales. They allow physicists to make meaningful predictions and advancements in understanding without requiring a complete or "perfect" model that works at all scales. Here’s a breakdown of how EFTs operate and their philosophical and practical underpinnings:

### Philosophy and Strategy of EFTs

1. **Limited Validity**: Each EFT is constructed to be valid only up to a certain energy scale, known as the cutoff. Beyond this scale, new physics may appear, and the EFT may no longer provide accurate predictions. This approach acknowledges our current limitations in accessing or understanding high-energy physics directly.

2. **Separation of Scales**: EFTs exploit the separation of scales—an assumption that high-energy processes can be decoupled from low-energy phenomena. This means the details at very high energies (or very short distances) do not significantly affect the physics observed at lower energies or larger distances, within the validity range of the EFT.

3. **Model Construction**: In constructing an EFT, physicists include all possible interactions that are consistent with the symmetries and constraints of the system up to the cutoff scale. This includes both renormalizable and non-renormalizable interactions. The latter are suppressed by powers of the ratio of the energy scale of the process being studied to the cutoff scale, reflecting their decreasing relevance at lower energies.

### Practical Application and Upgrading Models

1. **Validation and Calibration**: EFTs are tested by comparing their predictions with experimental or observational data within their valid energy range. If the EFT provides a good match to the data, it is used as a working model for further predictions and experiments.

2. **Sensitivity to New Physics**: One of the key uses of an EFT is to probe for signs of new physics. If discrepancies arise between the EFT predictions and experimental data, it may indicate the presence of physical effects that are not accounted for by the EFT, suggesting a need to modify the theory or to extend the energy range considered.

3. **Iterative Refinement**: As you mentioned, once an EFT is validated in a certain range and if new data or theoretical insights become available, the model can be refined or upgraded. This might involve adjusting the parameters, extending the range of validity, or incorporating additional interactions that were previously neglected.

### Philosophical Implications

The use of EFTs reflects a philosophical stance in modern physics that perfect, complete descriptions of nature might be unattainable or impractical but that progress can still be made by constructing theories that are effective within limited domains. This approach champions pragmatism and incremental progress in scientific understanding, acknowledging both our current knowledge and its limitations.

### Conclusion

Thus, EFTs are **not just stopgap measures but are a fundamental aspect** of how contemporary physics approaches problems across various domains, from particle physics and cosmology to condensed matter physics. They allow scientists to continue making precise, testable predictions even when the ultimate theories or complete descriptions of nature remain elusive.

## Other kinds of EFTs:

Effective Field Theories (EFTs) are a powerful framework used in theoretical physics to simplify problems by focusing on relevant phenomena at a chosen scale, while ignoring or summarizing details at higher energy scales through effective parameters. There are several types of EFTs used across various domains of physics, each tailored to address specific kinds of physical systems and scales. Here are some notable examples:

### 1. Chiral Perturbation Theory (ChPT)
Chiral Perturbation Theory is an EFT of Quantum Chromodynamics (QCD), the theory of the strong interaction, applicable at low energies. QCD becomes non-perturbative at low energies due to the increase of the strong coupling constant, making direct calculations impractical. ChPT deals with this by focusing on the interactions of the lightest mesons (like pions) and, in extended versions, nucleons. It exploits the chiral symmetry of QCD, which is spontaneously broken in nature, providing a systematic way to expand the observables in terms of a small parameter (the pion's momentum and mass compared to the QCD scale).

### 2. Heavy Quark Effective Theory (HQET)
HQET is another EFT derived from QCD, designed to simplify the study of hadrons containing heavy quarks (such as charm or bottom quarks). In HQET, the large mass of the heavy quark provides a natural separation of scales: the dynamics associated with the heavy quark mass are treated differently from those involving lighter degrees of freedom. This theory simplifies calculations by focusing only on the interactions that significantly affect the behavior of the heavy quark at lower energies.

### 3. Soft-Collinear Effective Theory (SCET)
SCET is an EFT for dealing with problems in QCD involving highly energetic particles moving close to the light cone. It is particularly useful in describing the dynamics of particles in processes like B-meson decays, where particles can have large energies but small invariant mass. SCET organizes the contributions to these processes according to their scaling in the small parameters (such as the ratio of transverse to longitudinal momentum components), allowing for systematic computations of scattering amplitudes.

### 4. Non-Relativistic QCD (NRQCD)
NRQCD is an EFT that simplifies the study of systems containing non-relativistic quarks, such as quarkonium (bound states of a heavy quark and its antiquark, such as charmonium and bottomonium). It separates the effects of short-distance interactions, which can be treated perturbatively, from long-distance effects, which are encoded in non-perturbative matrix elements. This separation is crucial for making accurate predictions about the production and decay of quarkonium states in high-energy physics experiments.

### 5. Gravitational EFT
In the context of gravity, EFT methods can be applied to General Relativity to study phenomena where the typical energy scales are much lower than the Planck scale. For example, EFT techniques are used to study gravitational waves and their interactions with matter, allowing physicists to systematically incorporate corrections from higher-order curvature terms or quantum effects.

### 6. Pionless Effective Field Theory
This is an EFT used in nuclear physics to describe low-energy nucleon-nucleon interactions without explicitly involving pions, which are the typical mediators of the nuclear force at slightly higher energies. This theory is applicable for processes at very low energies, typical of many nuclear physics experiments, where the details of the force carriers (like pions) can be ignored.

Each of these EFTs provides a framework where the infinite complexities of the underlying fundamental theories can be effectively managed by focusing on the most relevant phenomena at a given scale. This approach not only simplifies calculations but also enhances our understanding of the physical behavior across different scales, making it a cornerstone method in modern theoretical physics.


----
